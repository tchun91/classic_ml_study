
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>TF-IDF &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!-- 
    this give us a css class that will be invisible only if js is disabled 
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=26a4bc78f4c0ddb94549" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=26a4bc78f4c0ddb94549"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week5/TF-IDF, BoW, Faiss';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Set Up Hugging Face Space Environment" href="Transformer%2C_LoRA.html" />
    <link rel="prev" title="1. What are the Training types of each encoder and decoder models?" href="3.%20finetune_practice.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="../_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../week1/1.NaiveBayes.html">What is Naive Bayes Classifier?</a></li>

<li class="toctree-l1"><a class="reference internal" href="../week1/2.KNN.html">K-Nearest Neighbor(KNN) Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week1/Linear_Regression.html"><strong>Linear Regression</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../week1/Logistic_Regression.html">Logistic Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week1/Regularization.html">L1 and L2 Regularization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../week2/1.DecisionTree.html">What is Decision Tree?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week2/2.PCA.html">What is Principal Component Analysis?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week2/3.Hierarchical%20Clustering.html">What is Hierarchical Clustering?</a></li>

<li class="toctree-l1"><a class="reference internal" href="../week2/Gradient%20Boosting.html"><strong>Gradient Boosting Overview</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../week2/Kmeans.html">K-means Clustering</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../week3/1.svm.html">What is Support Vector Machine Algorithm?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../week3/CatBoost.html">CatBoost: Concept and Mathematical Implementation</a></li>

<li class="toctree-l1"><a class="reference internal" href="../week3/Random_Forest.html">Random forest</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../week4/1.collaborative_filtering.html">What is Collaborative Filtering?</a></li>


<li class="toctree-l1"><a class="reference internal" href="../week4/Market_Basket_Analysis.html">Market Basket Analysis (MBA)</a></li>


<li class="toctree-l1"><a class="reference internal" href="../week4/NDCG.html"><strong>Understanding NDCG (Normalized Discounted Cumulative Gain)</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="../week4/Semantic%20Search.html">1. Introduction to Semantic Search</a></li>




</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Week 5</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="3.%20finetune_practice.html">1. What are the Training types of each encoder and decoder models?</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">TF-IDF</a></li>


<li class="toctree-l1"><a class="reference internal" href="Transformer%2C_LoRA.html">Set Up Hugging Face Space Environment</a></li>



</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fweek5/TF-IDF, BoW, Faiss.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/week5/TF-IDF, BoW, Faiss.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>TF-IDF</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">TF-IDF</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concept">Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application">Application</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words-bow">Bag of Words (BoW)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-extraction-example">Feature Extraction Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages">Advantages and Disadvantages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-vectorization">Feature Vectorization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#faiss-facebook-ai-similarity-search">FAISS: Facebook AI Similarity Search</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundations-of-faiss">Mathematical Foundations of FAISS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-measures-and-distance-metrics">1. Similarity Measures and Distance Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-euclidean-distance-l2-norm">a. Euclidean Distance (L2 Norm)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-inner-product-dot-product">b. Inner Product (Dot Product)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nearest-neighbor-search">2. Nearest Neighbor Search</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges">Challenges:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-structures-and-algorithms">3. Indexing Structures and Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-product-quantization-pq">a. Product Quantization (PQ)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-inverted-file-index-ivf">b. Inverted File Index (IVF)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#c-hierarchical-navigable-small-world-graphs-hnsw">c. Hierarchical Navigable Small World Graphs (HNSW)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#d-optimized-product-quantization-opq">d. Optimized Product Quantization (OPQ)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tf-idf">
<h1>TF-IDF<a class="headerlink" href="#tf-idf" title="Link to this heading">#</a></h1>
<p><strong>TF-IDF</strong> (Term Frequency-Inverse Document Frequency) is a method used to calculate the importance of words in multiple documents. It helps determine the significance of words within a document and can be used to measure the similarity between documents.</p>
<section id="concept">
<h2>Concept<a class="headerlink" href="#concept" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>TF (Term Frequency)</strong>: Indicates how frequently a specific word appears in a document.
E.g., if the word “cat” appears 10 times in a document, the TF value is 10.</p></li>
<li><p><strong>IDF (Inverse Document Frequency)</strong>: Represents the inverse of how frequently a word appears across multiple documents. It is calculated as:</p>
<div class="math notranslate nohighlight">
\[
  \text{IDF} = \log\left(\frac{\text{Total number of documents}}{\text{Number of documents containing the word}}\right)
  \]</div>
<p>Words that appear frequently in many documents are considered less important, and IDF adjusts for this.</p>
</li>
<li><p><strong>TF-IDF</strong>: The product of TF and IDF values, which represents the importance of a specific word in a document. A high TF-IDF value indicates that the word appears often in a particular document but not in others, making it more significant.</p></li>
</ul>
</section>
<section id="application">
<h2>Application<a class="headerlink" href="#application" title="Link to this heading">#</a></h2>
<p>TF-IDF values are used to measure document similarity. Techniques like <strong>cosine similarity</strong> or <strong>clustering</strong> can be applied to determine how similar documents are to each other.</p>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h2>
<p>Consider three documents.</p>
<ol class="arabic simple">
<li><p>Tom plays soccer. (Doc1)</p></li>
<li><p>Tom loves soccer and baseball. (Doc2)</p></li>
<li><p>baseball is his hobby and his job. (Doc3)</p></li>
</ol>
<p><strong>TF Calculation</strong></p>
<p><img alt="Calculation of TF" src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_LxRJqXRjJNm95-R7B-xGQ.png" /></p>
<p><strong>IDF Calculation</strong></p>
<p><img alt="Calculation of TF" src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*AyiKGi5VfEikGdw_Tg9TdA.png" /></p>
<p>For instance, the word “Tom” has an IDF value of:</p>
<div class="math notranslate nohighlight">
\[
\text{IDF} = \log\left(\frac{3}{2}\right) \approx 0.18
\]</div>
<p><strong>TF-IDF Calculation</strong></p>
<p><img alt="Calculation of TF" src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*zHRhIUR7XOmVSIkZbuU_iA.png" /></p>
<p>The word “his” has a TF value of 2 and an IDF value of 0.48, resulting in a TF-IDF score of:</p>
<div class="math notranslate nohighlight">
\[
\text{TF-IDF} = 2 \times 0.48 = 0.96
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;stopwords&quot;</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[</span>
<span class="s2">&quot;The cat and the dog play in the garden&quot;</span><span class="p">,</span>
<span class="s2">&quot;The dog chases the cat around the house&quot;</span><span class="p">,</span>
<span class="s2">&quot;The house has a beautiful garden with flowers&quot;</span>
<span class="p">]</span>

<span class="n">tfidf_vectorizer</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">tfidf_matrix</span> <span class="o">=</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Word List:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">tfidf_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">TF-IDF Matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Doc </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">():</span>
        <span class="n">tf_idf_val</span> <span class="o">=</span> <span class="n">tfidf_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">tfidf_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">term</span><span class="p">]]</span>
        <span class="k">if</span> <span class="n">tf_idf_val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">term</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">tf_idf_val</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">nltk</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s2">&quot;stopwords&quot;</span><span class="p">,</span> <span class="n">quiet</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;nltk&#39;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="bag-of-words-bow">
<h1>Bag of Words (BoW)<a class="headerlink" href="#bag-of-words-bow" title="Link to this heading">#</a></h1>
<section id="id1">
<h2>Concept<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p><strong>Bag of Words (BoW)</strong> is a model that creates features by assigning frequency values to words without considering their order or context. The term “Bag of Words” comes from the concept of putting all the words from a document into a “bag” and mixing them up, ignoring their sequence or grammatical structure.</p>
</section>
<section id="feature-extraction-example">
<h2>Feature Extraction Example<a class="headerlink" href="#feature-extraction-example" title="Link to this heading">#</a></h2>
<p>Let’s consider the following three sentences as an example to illustrate BoW-based feature extraction:</p>
<ul class="simple">
<li><p>Sentence 1: <code class="docutils literal notranslate"><span class="pre">'Tom</span> <span class="pre">plays</span> <span class="pre">soccer'</span></code></p></li>
<li><p>Sentence 2: <code class="docutils literal notranslate"><span class="pre">'Tom</span> <span class="pre">loves</span> <span class="pre">soccer</span> <span class="pre">and</span> <span class="pre">baseball'</span></code></p></li>
<li><p>Sentence 3: <code class="docutils literal notranslate"><span class="pre">'baseball</span> <span class="pre">is</span> <span class="pre">his</span> <span class="pre">hobby</span> <span class="pre">and</span> <span class="pre">his</span> <span class="pre">job'</span></code></p></li>
</ul>
<ol class="arabic">
<li><p><strong>List Words and Assign Index</strong><br />
First, list all the unique words from these sentences and assign each word a unique index:</p>
<p>‘Tom’: 0, ‘plays’: 1, ‘soccer’: 2, ‘loves’: 3, ‘and’: 4, ‘baseball’: 5, ‘is’: 6, ‘his’: 7, ‘hobby’: 8, ‘job’: 9</p>
</li>
<li><p><strong>Frequency Vector for Each Sentence</strong><br />
Next, create a vector for each sentence based on the frequency of each word:</p></li>
</ol>
<ul class="simple">
<li><p>Sentence 1: <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p></li>
<li><p>Sentence 2: <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p></li>
<li><p>Sentence 3: <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">1,</span> <span class="pre">1]</span></code></p></li>
</ul>
<p>In these vectors, each value represents the frequency of the corresponding word in that sentence.</p>
</section>
<section id="advantages-and-disadvantages">
<h2>Advantages and Disadvantages<a class="headerlink" href="#advantages-and-disadvantages" title="Link to this heading">#</a></h2>
<p><strong>Advantages</strong>: Simple and effective in capturing the overall representation of a document.</p>
<p><strong>Disadvantages</strong>:</p>
<ul class="simple">
<li><p><strong>Lack of Semantic Context</strong>: BoW ignores the order of words, which means that it cannot understand the context or relationships between words.</p></li>
<li><p><strong>Sparsity Problem</strong>: The feature vectors created by BoW tend to be sparse, meaning that most of the values in the matrix are zeros. This is known as a <strong>sparse matrix</strong> and can negatively impact machine learning performance.</p></li>
</ul>
</section>
<section id="feature-vectorization">
<h2>Feature Vectorization<a class="headerlink" href="#feature-vectorization" title="Link to this heading">#</a></h2>
<p>process of converting textual data into numerical vectors that can be used by machine learning models. In BoW, this is done by listing all the words from the documents and assigning frequency values to them. If we have M documents and N unique words across those documents, the BoW-based feature vector will result in an M x N matrix.</p>
<p>BoW feature vectorization can be performed using two main approaches:</p>
<ul class="simple">
<li><p><strong>Count-based vectorization (CountVectorizer)</strong></p></li>
<li><p><strong>TF-IDF (Term Frequency-Inverse Document Frequency) vectorization</strong></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">X</span> <span class="o">=</span> <span class="p">[</span>
<span class="s2">&quot;The cat and the dog play in the garden&quot;</span><span class="p">,</span>
<span class="s2">&quot;The dog chases the cat around the house&quot;</span><span class="p">,</span>
<span class="s2">&quot;The house has a beautiful garden with flowers&quot;</span>
<span class="p">]</span>

<span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">stop_words</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">)</span>
<span class="n">count_matrix</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">count_matrix</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="n">count_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Word List:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">count_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Count Matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Doc </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">():</span>
        <span class="n">count_val</span> <span class="o">=</span> <span class="n">count_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">[</span><span class="n">term</span><span class="p">]]</span>
        <span class="k">if</span> <span class="n">count_val</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">term</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">count_val</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> Word List:
[&#39;beautiful&#39; &#39;cat&#39; &#39;chases&#39; &#39;dog&#39; &#39;flowers&#39; &#39;garden&#39; &#39;house&#39; &#39;play&#39;]

Count Matrix:
   beautiful  cat  chases  dog  flowers  garden  house  play
0          0    1       0    1        0       1      0     1
1          0    1       1    1        0       0      1     0
2          1    0       0    0        1       1      1     0

Doc 1: The cat and the dog play in the garden
  cat: 1
  dog: 1
  garden: 1
  play: 1

Doc 2: The dog chases the cat around the house
  cat: 1
  chases: 1
  dog: 1
  house: 1

Doc 3: The house has a beautiful garden with flowers
  beautiful: 1
  flowers: 1
  garden: 1
  house: 1
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="faiss-facebook-ai-similarity-search">
<h1>FAISS: Facebook AI Similarity Search<a class="headerlink" href="#faiss-facebook-ai-similarity-search" title="Link to this heading">#</a></h1>
<p>FAISS is an open-source library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. It is particularly designed to handle large-scale datasets containing high-dimensional vectors.</p>
<ul class="simple">
<li><p><strong>High Performance</strong>: Leverages efficient algorithms and hardware acceleration (CPU and GPU support) to achieve fast search times even on billions of vectors.</p></li>
<li><p><strong>Scalability</strong>: Designed to handle extremely large datasets efficiently.</p></li>
<li><p><strong>Versatility</strong>: Offers various indexing methods suitable for different data characteristics and performance requirements.</p></li>
</ul>
<section id="mathematical-foundations-of-faiss">
<h2>Mathematical Foundations of FAISS<a class="headerlink" href="#mathematical-foundations-of-faiss" title="Link to this heading">#</a></h2>
<p>The core functionality of FAISS revolves around efficient computation of vector similarities or distances and organizing data structures to facilitate rapid search.</p>
<section id="similarity-measures-and-distance-metrics">
<h3>1. Similarity Measures and Distance Metrics<a class="headerlink" href="#similarity-measures-and-distance-metrics" title="Link to this heading">#</a></h3>
<section id="a-euclidean-distance-l2-norm">
<h4>a. Euclidean Distance (L2 Norm)<a class="headerlink" href="#a-euclidean-distance-l2-norm" title="Link to this heading">#</a></h4>
<p>The Euclidean distance between two vectors <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
d_{\text{L2}}(\mathbf{x}, \mathbf{y}) = \left\| \mathbf{x} - \mathbf{y} \right\|_2 = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
\]</div>
<p>This metric measures the straight-line distance between two points in Euclidean space.</p>
</section>
<section id="b-inner-product-dot-product">
<h4>b. Inner Product (Dot Product)<a class="headerlink" href="#b-inner-product-dot-product" title="Link to this heading">#</a></h4>
<p>The inner product similarity between two vectors is:</p>
<div class="math notranslate nohighlight">
\[
s_{\text{dot}}(\mathbf{x}, \mathbf{y}) = \mathbf{x}^\top \mathbf{y} = \sum_{i=1}^n x_i y_i
\]</div>
<p>This measure is often used when vectors are normalized, relating closely to cosine similarity.</p>
</section>
</section>
<section id="nearest-neighbor-search">
<h3>2. Nearest Neighbor Search<a class="headerlink" href="#nearest-neighbor-search" title="Link to this heading">#</a></h3>
<p>The primary problem FAISS addresses is the <strong>Nearest Neighbor Search (NNS)</strong>:</p>
<p>Given a dataset <span class="math notranslate nohighlight">\(\mathcal{X} = \{\mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N\}\)</span> and a query vector <span class="math notranslate nohighlight">\(\mathbf{q}\)</span>, find the vector(s) in <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> closest to <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> according to a specified distance metric.</p>
<section id="challenges">
<h4>Challenges:<a class="headerlink" href="#challenges" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Curse of Dimensionality</strong>: As the dimensionality <span class="math notranslate nohighlight">\(n\)</span> increases, the computational cost of exact search becomes prohibitive.</p></li>
<li><p><strong>Large Datasets</strong>: With massive <span class="math notranslate nohighlight">\(N\)</span>, linear search is impractical.</p></li>
</ul>
<p><strong>Solution</strong>: FAISS employs Approximate Nearest Neighbor (ANN) algorithms that trade off a bit of accuracy for significant gains in efficiency.</p>
</section>
</section>
<section id="indexing-structures-and-algorithms">
<h3>3. Indexing Structures and Algorithms<a class="headerlink" href="#indexing-structures-and-algorithms" title="Link to this heading">#</a></h3>
<p>FAISS provides several indexing structures to accelerate search. Below are the key mathematical concepts behind these structures.</p>
<section id="a-product-quantization-pq">
<h4>a. Product Quantization (PQ)<a class="headerlink" href="#a-product-quantization-pq" title="Link to this heading">#</a></h4>
<p><strong>Objective</strong>: Compress high-dimensional vectors to reduce memory usage and accelerate distance computations.</p>
<p><strong>Method</strong>:</p>
<ol class="arabic">
<li><p><strong>Vector Subspace Decomposition</strong>:
Split the original <span class="math notranslate nohighlight">\(n\)</span>-dimensional space into <span class="math notranslate nohighlight">\(m\)</span> lower-dimensional subspaces. Each vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is partitioned:</p>
<div class="math notranslate nohighlight">
\[
   \mathbf{x} = [\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, \dots, \mathbf{x}^{(m)}]
   \]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)} \in \mathbb{R}^{n/m}\)</span>.</p>
</li>
<li><p><strong>Subspace Quantization</strong>:
For each subspace, build a codebook by clustering (e.g., using <span class="math notranslate nohighlight">\(k\)</span>-means):</p>
<ul class="simple">
<li><p>For subspace <span class="math notranslate nohighlight">\(i\)</span>, find <span class="math notranslate nohighlight">\(k\)</span> cluster centers <span class="math notranslate nohighlight">\(\{\mathbf{c}_1^{(i)}, \dots, \mathbf{c}_k^{(i)}\}\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Encoding Vectors</strong>:
Each sub-vector <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> is quantized to the nearest centroid:</p>
<div class="math notranslate nohighlight">
\[
   q^{(i)}(\mathbf{x}^{(i)}) = \arg\min_{j} \left\| \mathbf{x}^{(i)} - \mathbf{c}_j^{(i)} \right\|_2
   \]</div>
<p>The original vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is then represented by the set of indices <span class="math notranslate nohighlight">\([q^{(1)}(\mathbf{x}^{(1)}), \dots, q^{(m)}(\mathbf{x}^{(m)})]\)</span>.</p>
</li>
</ol>
<p><strong>Distance Approximation</strong>:</p>
<p>During search, the distance between a query <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> and a database vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is approximated as:</p>
<div class="math notranslate nohighlight">
\[
d_{\text{PQ}}(\mathbf{q}, \mathbf{x}) \approx \sum_{i=1}^m \left\| \mathbf{q}^{(i)} - \mathbf{c}_{q^{(i)}(\mathbf{x}^{(i)})}^{(i)} \right\|_2^2
\]</div>
<p>Precomputing the distances between query sub-vectors and codebook centroids allows efficient computation.</p>
</section>
<section id="b-inverted-file-index-ivf">
<h4>b. Inverted File Index (IVF)<a class="headerlink" href="#b-inverted-file-index-ivf" title="Link to this heading">#</a></h4>
<p><strong>Objective</strong>: Reduce the number of candidate vectors to consider during search by partitioning the dataset.</p>
<p><strong>Method</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Coarse Quantization</strong>:
Cluster the entire dataset into <span class="math notranslate nohighlight">\(K\)</span> coarse clusters using <span class="math notranslate nohighlight">\(k\)</span>-means clustering in the original space. Each cluster corresponds to an inverted list.</p></li>
<li><p><strong>Assignment</strong>:
Assign each database vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to its nearest coarse centroid <span class="math notranslate nohighlight">\(\mathbf{C}(\mathbf{x})\)</span>.</p></li>
<li><p><strong>Search</strong>:</p>
<ul class="simple">
<li><p><strong>Query Assignment</strong>: Assign the query <span class="math notranslate nohighlight">\(\mathbf{q}\)</span> to its <span class="math notranslate nohighlight">\(L\)</span> nearest coarse centroids.</p></li>
<li><p><strong>Candidate Retrieval</strong>: Consider only the vectors in the corresponding <span class="math notranslate nohighlight">\(L\)</span> inverted lists for further evaluation.</p></li>
</ul>
</li>
</ol>
</section>
<section id="c-hierarchical-navigable-small-world-graphs-hnsw">
<h4>c. Hierarchical Navigable Small World Graphs (HNSW)<a class="headerlink" href="#c-hierarchical-navigable-small-world-graphs-hnsw" title="Link to this heading">#</a></h4>
<p><strong>Objective</strong>: Provide efficient approximate nearest neighbor search using a graph-based approach.</p>
<p><strong>Method</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Graph Construction</strong>:</p>
<ul class="simple">
<li><p>Build a hierarchical graph with multiple layers.</p></li>
<li><p><strong>Upper Layers</strong>: Contain a subset of the data points with long-range links.</p></li>
<li><p><strong>Lower Layers</strong>: Include all data points with links to nearby neighbors.</p></li>
</ul>
</li>
<li><p><strong>Navigability</strong>:</p>
<ul class="simple">
<li><p>Edges are created between nodes based on proximity, using heuristics to maintain a “small-world” property, ensuring logarithmic search time.</p></li>
</ul>
</li>
<li><p><strong>Search Algorithm</strong>:</p>
<ul class="simple">
<li><p><strong>Greedy Search</strong>: Starting from an entry point at the top layer, iteratively move to neighbors closer to the query <span class="math notranslate nohighlight">\(\mathbf{q}\)</span>.</p></li>
<li><p><strong>Layer Traversal</strong>: Once the lowest layer is reached, the search continues within that layer to refine the nearest neighbors.</p></li>
</ul>
</li>
</ol>
</section>
<section id="d-optimized-product-quantization-opq">
<h4>d. Optimized Product Quantization (OPQ)<a class="headerlink" href="#d-optimized-product-quantization-opq" title="Link to this heading">#</a></h4>
<p>An extension of PQ that applies a rotation (or linear transformation) to the data before quantization to minimize quantization error.</p>
<p><strong>Mathematical Formulation</strong>:</p>
<ol class="arabic">
<li><p><strong>Find Optimal Rotation Matrix</strong> <span class="math notranslate nohighlight">\(\mathbf{R}\)</span>:
Solve:</p>
<div class="math notranslate nohighlight">
\[
   \min_{\mathbf{R}} \sum_{i=1}^N \left\| \mathbf{x}_i - \mathbf{R}^\top q(\mathbf{R} \mathbf{x}_i) \right\|_2^2
   \]</div>
<p>where <span class="math notranslate nohighlight">\(q(\cdot)\)</span> is the quantization function.</p>
</li>
<li><p><strong>Rotation and Quantization</strong>:</p>
<ul class="simple">
<li><p>Rotate vectors: <span class="math notranslate nohighlight">\(\mathbf{z}_i = \mathbf{R} \mathbf{x}_i\)</span>.</p></li>
<li><p>Apply PQ on rotated vectors <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span>.</p></li>
</ul>
</li>
</ol>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./week5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3.%20finetune_practice.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">1. What are the Training types of each encoder and decoder models?</p>
      </div>
    </a>
    <a class="right-next"
       href="Transformer%2C_LoRA.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Set Up Hugging Face Space Environment</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">TF-IDF</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concept">Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#application">Application</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#bag-of-words-bow">Bag of Words (BoW)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Concept</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-extraction-example">Feature Extraction Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-and-disadvantages">Advantages and Disadvantages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-vectorization">Feature Vectorization</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#faiss-facebook-ai-similarity-search">FAISS: Facebook AI Similarity Search</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundations-of-faiss">Mathematical Foundations of FAISS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-measures-and-distance-metrics">1. Similarity Measures and Distance Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-euclidean-distance-l2-norm">a. Euclidean Distance (L2 Norm)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-inner-product-dot-product">b. Inner Product (Dot Product)</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#nearest-neighbor-search">2. Nearest Neighbor Search</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges">Challenges:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#indexing-structures-and-algorithms">3. Indexing Structures and Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#a-product-quantization-pq">a. Product Quantization (PQ)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#b-inverted-file-index-ivf">b. Inverted File Index (IVF)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#c-hierarchical-navigable-small-world-graphs-hnsw">c. Hierarchical Navigable Small World Graphs (HNSW)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#d-optimized-product-quantization-opq">d. Optimized Product Quantization (OPQ)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=26a4bc78f4c0ddb94549"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=26a4bc78f4c0ddb94549"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
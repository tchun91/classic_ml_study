{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38b422f-b086-428f-aee6-1b1ee1f31cdf",
   "metadata": {},
   "source": [
    "## What is Support Vector Machine Algorithm?\n",
    "- Powerful ML for regression and classification tasks, but best suited for classification\n",
    "- Tries to find the maximum separating hyperplane between the different classes available in the target\n",
    "- Finding the optimal hyperplane in an N-dimensional space that can separate data points in different classes in feature space.\n",
    "<div>\n",
    "<img src=\"svm_hyperplane.png\" width=\"300\">\n",
    "</div>\n",
    "- 2-D plane if you have three features.\n",
    "- SVM is similar to decision tree that treis to find a best split, but rather focused on hyperplane in more than one feature.\n",
    "- The hyperplane treis that the margin between closest points of different classes should be as max as possible. \n",
    "- SVM works the best when you think you have data that can be segregated, but their relationships are not linear. ex) \n",
    "\n",
    "### 1. How does SVM work?\n",
    "- Choose the hyperplane that has the nearest data point on each side is maximized. (Similar to linear regression fitted line)\n",
    "- Hyperplane can ignore outliers\n",
    "- SVM tries to minimize $ \\frac{1}{margin} + (\\sum penalty) $ where margin is the distance between closest datapoints from each class\n",
    "- Hinge loss is commonly used.\n",
    "\n",
    "### 2. Terminology of SVM\n",
    "1) <b>Hyperplance</b> : linear equation - wx+b = 0\n",
    "- $ w^Tx + b = 0 $\n",
    "- $ d_i = \\frac{w^Tx_i+b}{||w||} $\n",
    "- $ f(x) = \n",
    "\\begin{cases} \n",
    "1 : w^Tx + b >=0 \\\\\n",
    "0 : w^Tx + b <0\n",
    "\\end{cases} $\n",
    "\n",
    "2) <b>Weight Vectors</b> : defines the orientation of hyperplane separating the classes.\n",
    "\n",
    "3) <b>Support Vectors</b> : closest data points to the hyperplane\n",
    "  \n",
    "4) <b>Margin</b> : distance between sv and hyperplane\n",
    "  \n",
    "5) <b>kernel</b> : math function used to map the original input data points into high-dimensional feature spaces. It helps to find the best hyperplane if the data points are not linearly separable in the original input space. Fns are linear, polynomial, radial basis, and sigmoid.\n",
    "  \n",
    "6) <b>Hard Margin</b> : Separates the data points of each class without any misclassification\n",
    "- $ minimize_{w,b} \\frac{1}{2}w^Tw = minimize_{w,b} \\frac{1}{2} ||w||^2 $\n",
    "- subject to $ y_i(w^Tx_i + b)>= 1 $ for i = 1,2,3,...,m\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7) <b>Soft Margin</b> : This technique helps when the data is not perfectly separable by adopting slack variable in each data point. This slack variable smoothens the margin requirement and allow some violations. There is a balance trade off between increasing margin and reducing violation.\n",
    "- $ \\min_{w, b, \\xi} \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i $\n",
    "- $ \\text{subject to:} y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\forall i $\n",
    "- $ \\xi_i \\geq 0, \\forall i $\n",
    "\n",
    "When it cannot be spliited because it is not linear.\n",
    "<div>\n",
    "<img src=\"svm_hard_case.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "Kernel tricks can split the data.\n",
    "<div>\n",
    "<img src=\"svm_kernel_applied.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "8) <b>C</b> : Regularization parameter C is a penalty term that stricts between misclassification and margin.\n",
    "9) <b>Hinge Loss</b> : penlizes the misclassification and points that are correctly classified, but too close to the decision boundary.\n",
    "10) <b>Dual Problem</b> : Optimization problem - Lagrange multipliers to solve SVM. Dual formulation enables to use kernel tricks and effective computing.\n",
    "- Here $\\alpha_i$ is Lagrange multiplier to find the local min/max and associated with the data points representing importance of each data points.\n",
    "- $\n",
    "\\max_{\\alpha} \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)\n",
    "$\n",
    "- $\n",
    "\\text{subject to:} 0 \\leq \\alpha_i \\leq C, \\quad \\forall i\n",
    "$\n",
    "- $\n",
    "\\sum_{i=1}^{n} \\alpha_i y_i = 0\n",
    "$\n",
    "- $ f(x) = \\sum_{i=1}^{n} \\alpha_i y_i K(x_i, x) + b $\n",
    "\n",
    "### 3. Procedure\n",
    "1. Normalize/standardize\n",
    "2. Select model type either hard-margin/soft-margin\n",
    "3. Choose kernel for non-linear classification; liner, polynomial, RBF(Gaussian)\n",
    "4. Optimization problem to find the best optimal hyperplane\n",
    "5. Make prediction\n",
    "\n",
    "### 4. Advanatages over other modesl\n",
    "1. Effective in high-dimensional cases.\n",
    "2. Its memory is efficient as it uses a subset of training points in the decision function called support vectors.\n",
    "3. Different kernel functions can be specified for the decision functions and its possible to specify custom kernels.\n",
    "4. Well performing for high dimentional cases and disadvantages for large dataset\n",
    "- Reference Link : https://www.geeksforgeeks.org/support-vector-machine-algorithm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc8cc083-4b3a-4e7b-92d9-a270e2ba94bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers  # Used for quadratic programming\n",
    "\n",
    "# Define the polynomial kernel function\n",
    "def polynomial_kernel(x1, x2, degree=3, coef0=1):\n",
    "    return (np.dot(x1, x2) + coef0) ** degree\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, C=1.0, kernel=polynomial_kernel, degree=3, coef0=1):\n",
    "        self.C = C  # Regularization parameter\n",
    "        self.kernel = kernel  # Kernel function\n",
    "        self.degree = degree  # Degree of polynomial kernel\n",
    "        self.coef0 = coef0  # Coefficient for polynomial kernel\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Apply Kernel function\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j], self.degree, self.coef0)\n",
    "        \n",
    "        # Construct the matrices for the quadratic programming problem\n",
    "        #if y is (n,1) np.outer makes it (n,n) outer product\n",
    "        #(1,2,3) outer product (1,2,3) = ([1,2,3],[2,4,6],[3,6,9])\n",
    "        P = matrix(np.outer(y, y) * K) \n",
    "        q = matrix(-np.ones(n_samples))\n",
    "\n",
    "        #np.eye is diagonal matrix\n",
    "        #vstack concatenate matrix vertically\n",
    "        #hstack concatenate matrix vertically\n",
    "        G = matrix(np.vstack((-np.eye(n_samples), np.eye(n_samples))))\n",
    "        h = matrix(np.hstack((np.zeros(n_samples), np.ones(n_samples) * self.C)))\n",
    "        #A is y into matrix with 1xn\n",
    "        A = matrix(y, (1, n_samples), 'd')\n",
    "        b = matrix(0.0)\n",
    "\n",
    "        # Solve the quadratic programming problem using cvxopt\n",
    "        solvers.options['show_progress'] = False\n",
    "\n",
    "        #b is 0 on the right hand side\n",
    "        #P is kernal matrix\n",
    "        # q is -1 vector\n",
    "        # G is block matrix combination of identity matrices and negative identity matrices to handle contraints\n",
    "        # h is for upper and lower bound using C (Penalty regularization term in soft margin)\n",
    "        # A is a label vector (1xn)\n",
    "        solution = solvers.qp(P, q, G, h, A, b)\n",
    "        #Flattens the result matrix into 1d\n",
    "        alphas = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        #This masking operation filters out very small values of alpha\n",
    "        #1e-5 is used to select only effective data points to be considered in support vectors\n",
    "        sv = alphas > 1e-5\n",
    "        self.alphas = alphas[sv]\n",
    "        self.support_vectors_ = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "\n",
    "        # Weight vector calculation\n",
    "        self.w = np.sum(self.alphas[:, None] * self.sv_y[:, None] * self.support_vectors_, axis=0)\n",
    "\n",
    "        # Calculate bias\n",
    "        self.b = np.mean([y_k - np.sum(self.alphas * self.sv_y * K[sv_idx, sv])\n",
    "                          for sv_idx, y_k in zip(np.where(sv)[0], self.sv_y)])\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = np.sum(self.alphas * self.sv_y * np.array([self.kernel(sv, X, self.degree, self.coef0) for sv in self.support_vectors_]), axis=0) + self.b\n",
    "        return np.sign(y_pred)\n",
    "\n",
    "# Preprocessing Iris dataset\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:100, :2]  # Only two features for visualization, and two classes for binary classification\n",
    "y = iris.target[:100]\n",
    "y = np.where(y == 0, -1, 1)  # Convert to -1 and 1\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Train the SVM model with a polynomial kernel\n",
    "svm = SVM(C=1.0, kernel=polynomial_kernel, degree=3, coef0=1)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = np.array([svm.predict(x) for x in X_test])\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75447401-8d06-4cd9-aabd-984ca5e2072b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1,  1,  1, ...,  1, -1, -1]],\n",
       "\n",
       "       [[ 1,  1,  1, ...,  1, -1, -1]],\n",
       "\n",
       "       [[ 1,  1,  1, ...,  1, -1, -1]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1,  1,  1, ...,  1, -1, -1]],\n",
       "\n",
       "       [[-1, -1, -1, ..., -1,  1,  1]],\n",
       "\n",
       "       [[-1, -1, -1, ..., -1,  1,  1]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.outer(y_train,y_train)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "68923797-f63b-4761-88c0-b07aaa390283",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_shape0 = X_train.shape[0]\n",
    "K = np.zeros((s_shape0,s_shape0))\n",
    "for i in range(s_shape0):\n",
    "    for j in range(s_shape0):\n",
    "        K[i,j] = polynomial_kernel(X_train[i], X_train[j], 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "06ca91f9-68b7-4545-a089-ed468deda8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 60)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31d46e94-eaf2-417a-965c-9242c32580a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(y_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32718392-74a8-4edf-9777-56f70626415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<60x60 matrix, tc='d'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(np.outer(y_train,y_train)*K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52a9906f-bbba-4f33-a093-b57333b1c97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 1., 0., 0., 1.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ravel(np.vstack((np.eye(2),np.eye(2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3df5e71e-2267-4801-9a08-b06da30fe82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x60 matrix, tc='d'>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix(y_train, (1, 60), 'd')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e918ce7-b942-4212-ad36-04ec8700a395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [[-0.]\n",
      " [-1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "\n",
    "# Define the quadratic and linear parts of the objective function\n",
    "P = matrix([[1.0, 0.0], [1.0, 1.0]])  # Coefficients for x1^2 and x2^2\n",
    "q = matrix([0.0, 1.0])  # Coefficients for linear terms (none in this case)\n",
    "\n",
    "# No constraints: no G, h, A, b\n",
    "\n",
    "# Solve the quadratic programming problem\n",
    "solution = solvers.qp(P, q)\n",
    "\n",
    "# Extract the optimal solution\n",
    "x_opt = np.array(solution['x'])\n",
    "print(\"Optimal solution:\", x_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15ea46ef-7036-4bdf-abce-f6f7a4173dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1*1e-5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

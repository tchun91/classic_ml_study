{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f941df3-fe75-4f3c-a32a-e75b5845f549",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "**TF-IDF** (Term Frequency-Inverse Document Frequency) is a method used to calculate the importance of words in multiple documents. It helps determine the significance of words within a document and can be used to measure the similarity between documents.\n",
    "\n",
    "## Concept\n",
    "- **TF (Term Frequency)**: Indicates how frequently a specific word appears in a document.\n",
    "                            E.g., if the word \"cat\" appears 10 times in a document, the TF value is 10.\n",
    "  \n",
    "- **IDF (Inverse Document Frequency)**: Represents the inverse of how frequently a word appears across multiple documents. It is calculated as:\n",
    "\n",
    "  $$\n",
    "  \\text{IDF} = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}}\\right)\n",
    "  $$\n",
    "\n",
    "  Words that appear frequently in many documents are considered less important, and IDF adjusts for this.\n",
    "  \n",
    "- **TF-IDF**: The product of TF and IDF values, which represents the importance of a specific word in a document. A high TF-IDF value indicates that the word appears often in a particular document but not in others, making it more significant.\n",
    "\n",
    "## Application\n",
    "TF-IDF values are used to measure document similarity. Techniques like **cosine similarity** or **clustering** can be applied to determine how similar documents are to each other.\n",
    "\n",
    "## Example\n",
    "Consider three documents. \n",
    "1. Tom plays soccer. (Doc1)\n",
    "2. Tom loves soccer and baseball. (Doc2)\n",
    "3. baseball is his hobby and his job. (Doc3)\n",
    "\n",
    "**TF Calculation**\n",
    "\n",
    "![Calculation of TF](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_LxRJqXRjJNm95-R7B-xGQ.png)\n",
    "\n",
    "**IDF Calculation**\n",
    "\n",
    "![Calculation of TF](https://miro.medium.com/v2/resize:fit:828/format:webp/1*AyiKGi5VfEikGdw_Tg9TdA.png)\n",
    " \n",
    "For instance, the word \"Tom\" has an IDF value of:\n",
    "\n",
    "$$\n",
    "\\text{IDF} = \\log\\left(\\frac{3}{2}\\right) \\approx 0.18\n",
    "$$\n",
    "\n",
    "**TF-IDF Calculation**\n",
    "\n",
    "![Calculation of TF](https://miro.medium.com/v2/resize:fit:828/format:webp/1*zHRhIUR7XOmVSIkZbuU_iA.png)\n",
    "\n",
    "The word \"his\" has a TF value of 2 and an IDF value of 0.48, resulting in a TF-IDF score of:\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF} = 2 \\times 0.48 = 0.96\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb32aa86-7147-492e-bbc8-e22645a9929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Word List:\n",
      "['beautiful' 'cat' 'chases' 'dog' 'flowers' 'garden' 'house' 'play']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "   beautiful       cat    chases       dog   flowers    garden     house  \\\n",
      "0   0.000000  0.459854  0.000000  0.459854  0.000000  0.459854  0.000000   \n",
      "1   0.000000  0.459854  0.604652  0.459854  0.000000  0.000000  0.459854   \n",
      "2   0.562829  0.000000  0.000000  0.000000  0.562829  0.428046  0.428046   \n",
      "\n",
      "       play  \n",
      "0  0.604652  \n",
      "1  0.000000  \n",
      "2  0.000000  \n",
      "\n",
      "Doc 1: The cat and the dog play in the garden\n",
      "  cat: 0.4599\n",
      "  dog: 0.4599\n",
      "  garden: 0.4599\n",
      "  play: 0.6047\n",
      "\n",
      "Doc 2: The dog chases the cat around the house\n",
      "  cat: 0.4599\n",
      "  chases: 0.6047\n",
      "  dog: 0.4599\n",
      "  house: 0.4599\n",
      "\n",
      "Doc 3: The house has a beautiful garden with flowers\n",
      "  beautiful: 0.5628\n",
      "  flowers: 0.5628\n",
      "  garden: 0.4280\n",
      "  house: 0.4280\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = [\n",
    "\"The cat and the dog play in the garden\",\n",
    "\"The dog chases the cat around the house\",\n",
    "\"The house has a beautiful garden with flowers\"\n",
    "]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(\"\\n Word List:\")\n",
    "print(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(df)\n",
    "\n",
    "for i, doc in enumerate(X):\n",
    "    print(f\"\\nDoc {i+1}: {doc}\")\n",
    "    for term in tfidf_vectorizer.get_feature_names_out():\n",
    "        tf_idf_val = tfidf_matrix[i, tfidf_vectorizer.vocabulary_[term]]\n",
    "        if tf_idf_val > 0:\n",
    "            print(f\"  {term}: {tf_idf_val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c40084-e132-4536-a0d7-bf0c256fec2a",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb848c1-199e-4741-9e7d-6f37bdbdac90",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "\n",
    "### Concept\n",
    "**Bag of Words (BoW)** is a model that creates features by assigning frequency values to words without considering their order or context. The term \"Bag of Words\" comes from the concept of putting all the words from a document into a \"bag\" and mixing them up, ignoring their sequence or grammatical structure.\n",
    "\n",
    "### Feature Extraction Example\n",
    "Let's consider the following three sentences as an example to illustrate BoW-based feature extraction:\n",
    "\n",
    "- Sentence 1: `'Tom plays soccer'`\n",
    "- Sentence 2: `'Tom loves soccer and baseball'`\n",
    "- Sentence 3: `'baseball is his hobby and his job'`\n",
    "\n",
    "1. **List Words and Assign Index**  \n",
    "   First, list all the unique words from these sentences and assign each word a unique index:\n",
    "\n",
    "    'Tom': 0, 'plays': 1, 'soccer': 2, 'loves': 3, 'and': 4, 'baseball': 5, 'is': 6, 'his': 7, 'hobby': 8, 'job': 9\n",
    "\n",
    "2. **Frequency Vector for Each Sentence**  \n",
    "Next, create a vector for each sentence based on the frequency of each word:\n",
    "\n",
    "- Sentence 1: `[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]`\n",
    "- Sentence 2: `[1, 0, 1, 1, 1, 1, 0, 0, 0, 0]`\n",
    "- Sentence 3: `[0, 0, 0, 0, 0, 1, 1, 2, 1, 1]`\n",
    "\n",
    "In these vectors, each value represents the frequency of the corresponding word in that sentence.\n",
    "\n",
    "### Advantages and Disadvantages\n",
    "**Advantages**: Simple and effective in capturing the overall representation of a document.\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Lack of Semantic Context**: BoW ignores the order of words, which means that it cannot understand the context or relationships between words.\n",
    "- **Sparsity Problem**: The feature vectors created by BoW tend to be sparse, meaning that most of the values in the matrix are zeros. This is known as a **sparse matrix** and can negatively impact machine learning performance.\n",
    "\n",
    "### Feature Vectorization\n",
    "process of converting textual data into numerical vectors that can be used by machine learning models. In BoW, this is done by listing all the words from the documents and assigning frequency values to them. If we have M documents and N unique words across those documents, the BoW-based feature vector will result in an M x N matrix.\n",
    "\n",
    "BoW feature vectorization can be performed using two main approaches:\n",
    "- **Count-based vectorization (CountVectorizer)**\n",
    "- **TF-IDF (Term Frequency-Inverse Document Frequency) vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08cdd623-2c1c-455f-b4c2-e484c6e6ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Word List:\n",
      "['beautiful' 'cat' 'chases' 'dog' 'flowers' 'garden' 'house' 'play']\n",
      "\n",
      "Count Matrix:\n",
      "   beautiful  cat  chases  dog  flowers  garden  house  play\n",
      "0          0    1       0    1        0       1      0     1\n",
      "1          0    1       1    1        0       0      1     0\n",
      "2          1    0       0    0        1       1      1     0\n",
      "\n",
      "Doc 1: The cat and the dog play in the garden\n",
      "  cat: 1\n",
      "  dog: 1\n",
      "  garden: 1\n",
      "  play: 1\n",
      "\n",
      "Doc 2: The dog chases the cat around the house\n",
      "  cat: 1\n",
      "  chases: 1\n",
      "  dog: 1\n",
      "  house: 1\n",
      "\n",
      "Doc 3: The house has a beautiful garden with flowers\n",
      "  beautiful: 1\n",
      "  flowers: 1\n",
      "  garden: 1\n",
      "  house: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "X = [\n",
    "\"The cat and the dog play in the garden\",\n",
    "\"The dog chases the cat around the house\",\n",
    "\"The house has a beautiful garden with flowers\"\n",
    "]\n",
    "\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_matrix = count_vectorizer.fit_transform(X)\n",
    "\n",
    "df = pd.DataFrame(count_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\n Word List:\")\n",
    "print(count_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\nCount Matrix:\")\n",
    "print(df)\n",
    "\n",
    "for i, doc in enumerate(X):\n",
    "    print(f\"\\nDoc {i+1}: {doc}\")\n",
    "    for term in count_vectorizer.get_feature_names_out():\n",
    "        count_val = count_matrix[i, count_vectorizer.vocabulary_[term]]\n",
    "        if count_val > 0:\n",
    "            print(f\"  {term}: {count_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f1a01-08f5-4c0f-814a-5e2979434cf9",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fefb58-86a3-407f-b788-73a463f7cbb4",
   "metadata": {},
   "source": [
    "# FAISS: Facebook AI Similarity Search\n",
    "\n",
    "FAISS is an open-source library developed by Facebook AI Research for efficient similarity search and clustering of dense vectors. It is particularly designed to handle large-scale datasets containing high-dimensional vectors.\n",
    "\n",
    "- **High Performance**: Leverages efficient algorithms and hardware acceleration (CPU and GPU support) to achieve fast search times even on billions of vectors.\n",
    "- **Scalability**: Designed to handle extremely large datasets efficiently.\n",
    "- **Versatility**: Offers various indexing methods suitable for different data characteristics and performance requirements.\n",
    "\n",
    "## Mathematical Foundations of FAISS\n",
    "\n",
    "The core functionality of FAISS revolves around efficient computation of vector similarities or distances and organizing data structures to facilitate rapid search.\n",
    "\n",
    "### 1. Similarity Measures and Distance Metrics\n",
    "\n",
    "#### a. Euclidean Distance (L2 Norm)\n",
    "\n",
    "The Euclidean distance between two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ is given by:\n",
    "\n",
    "$$\n",
    "d_{\\text{L2}}(\\mathbf{x}, \\mathbf{y}) = \\left\\| \\mathbf{x} - \\mathbf{y} \\right\\|_2 = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "This metric measures the straight-line distance between two points in Euclidean space.\n",
    "\n",
    "#### b. Inner Product (Dot Product)\n",
    "\n",
    "The inner product similarity between two vectors is:\n",
    "\n",
    "$$\n",
    "s_{\\text{dot}}(\\mathbf{x}, \\mathbf{y}) = \\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^n x_i y_i\n",
    "$$\n",
    "\n",
    "This measure is often used when vectors are normalized, relating closely to cosine similarity.\n",
    "\n",
    "### 2. Nearest Neighbor Search\n",
    "\n",
    "The primary problem FAISS addresses is the **Nearest Neighbor Search (NNS)**:\n",
    "\n",
    "Given a dataset $\\mathcal{X} = \\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_N\\}$ and a query vector $\\mathbf{q}$, find the vector(s) in $\\mathcal{X}$ closest to $\\mathbf{q}$ according to a specified distance metric.\n",
    "\n",
    "#### Challenges:\n",
    "- **Curse of Dimensionality**: As the dimensionality $n$ increases, the computational cost of exact search becomes prohibitive.\n",
    "- **Large Datasets**: With massive $N$, linear search is impractical.\n",
    "\n",
    "**Solution**: FAISS employs Approximate Nearest Neighbor (ANN) algorithms that trade off a bit of accuracy for significant gains in efficiency.\n",
    "\n",
    "### 3. Indexing Structures and Algorithms\n",
    "\n",
    "FAISS provides several indexing structures to accelerate search. Below are the key mathematical concepts behind these structures.\n",
    "\n",
    "#### a. Product Quantization (PQ)\n",
    "\n",
    "**Objective**: Compress high-dimensional vectors to reduce memory usage and accelerate distance computations.\n",
    "\n",
    "**Method**:\n",
    "\n",
    "1. **Vector Subspace Decomposition**:\n",
    "   Split the original $n$-dimensional space into $m$ lower-dimensional subspaces. Each vector $\\mathbf{x}$ is partitioned:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{x} = [\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, \\dots, \\mathbf{x}^{(m)}]\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{n/m}$.\n",
    "\n",
    "2. **Subspace Quantization**:\n",
    "   For each subspace, build a codebook by clustering (e.g., using $k$-means):\n",
    "   - For subspace $i$, find $k$ cluster centers $\\{\\mathbf{c}_1^{(i)}, \\dots, \\mathbf{c}_k^{(i)}\\}$.\n",
    "\n",
    "3. **Encoding Vectors**:\n",
    "   Each sub-vector $\\mathbf{x}^{(i)}$ is quantized to the nearest centroid:\n",
    "\n",
    "   $$\n",
    "   q^{(i)}(\\mathbf{x}^{(i)}) = \\arg\\min_{j} \\left\\| \\mathbf{x}^{(i)} - \\mathbf{c}_j^{(i)} \\right\\|_2\n",
    "   $$\n",
    "\n",
    "   The original vector $\\mathbf{x}$ is then represented by the set of indices $[q^{(1)}(\\mathbf{x}^{(1)}), \\dots, q^{(m)}(\\mathbf{x}^{(m)})]$.\n",
    "\n",
    "**Distance Approximation**:\n",
    "\n",
    "During search, the distance between a query $\\mathbf{q}$ and a database vector $\\mathbf{x}$ is approximated as:\n",
    "\n",
    "$$\n",
    "d_{\\text{PQ}}(\\mathbf{q}, \\mathbf{x}) \\approx \\sum_{i=1}^m \\left\\| \\mathbf{q}^{(i)} - \\mathbf{c}_{q^{(i)}(\\mathbf{x}^{(i)})}^{(i)} \\right\\|_2^2\n",
    "$$\n",
    "\n",
    "Precomputing the distances between query sub-vectors and codebook centroids allows efficient computation.\n",
    "\n",
    "#### b. Inverted File Index (IVF)\n",
    "\n",
    "**Objective**: Reduce the number of candidate vectors to consider during search by partitioning the dataset.\n",
    "\n",
    "**Method**:\n",
    "\n",
    "1. **Coarse Quantization**:\n",
    "   Cluster the entire dataset into $K$ coarse clusters using $k$-means clustering in the original space. Each cluster corresponds to an inverted list.\n",
    "\n",
    "2. **Assignment**:\n",
    "   Assign each database vector $\\mathbf{x}$ to its nearest coarse centroid $\\mathbf{C}(\\mathbf{x})$.\n",
    "\n",
    "3. **Search**:\n",
    "   - **Query Assignment**: Assign the query $\\mathbf{q}$ to its $L$ nearest coarse centroids.\n",
    "   - **Candidate Retrieval**: Consider only the vectors in the corresponding $L$ inverted lists for further evaluation.\n",
    "\n",
    "#### c. Hierarchical Navigable Small World Graphs (HNSW)\n",
    "\n",
    "**Objective**: Provide efficient approximate nearest neighbor search using a graph-based approach.\n",
    "\n",
    "**Method**:\n",
    "\n",
    "1. **Graph Construction**:\n",
    "   - Build a hierarchical graph with multiple layers.\n",
    "   - **Upper Layers**: Contain a subset of the data points with long-range links.\n",
    "   - **Lower Layers**: Include all data points with links to nearby neighbors.\n",
    "\n",
    "2. **Navigability**:\n",
    "   - Edges are created between nodes based on proximity, using heuristics to maintain a \"small-world\" property, ensuring logarithmic search time.\n",
    "\n",
    "3. **Search Algorithm**:\n",
    "   - **Greedy Search**: Starting from an entry point at the top layer, iteratively move to neighbors closer to the query $\\mathbf{q}$.\n",
    "   - **Layer Traversal**: Once the lowest layer is reached, the search continues within that layer to refine the nearest neighbors.\n",
    "\n",
    "#### d. Optimized Product Quantization (OPQ)\n",
    "\n",
    "An extension of PQ that applies a rotation (or linear transformation) to the data before quantization to minimize quantization error.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "\n",
    "1. **Find Optimal Rotation Matrix** $\\mathbf{R}$:\n",
    "   Solve:\n",
    "\n",
    "   $$\n",
    "   \\min_{\\mathbf{R}} \\sum_{i=1}^N \\left\\| \\mathbf{x}_i - \\mathbf{R}^\\top q(\\mathbf{R} \\mathbf{x}_i) \\right\\|_2^2\n",
    "   $$\n",
    "\n",
    "   where $q(\\cdot)$ is the quantization function.\n",
    "\n",
    "2. **Rotation and Quantization**:\n",
    "   - Rotate vectors: $\\mathbf{z}_i = \\mathbf{R} \\mathbf{x}_i$.\n",
    "   - Apply PQ on rotated vectors $\\mathbf{z}_i$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

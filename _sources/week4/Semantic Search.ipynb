{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f123c58-1987-44af-8a32-c295be753fed",
   "metadata": {},
   "source": [
    "## 1. Introduction to Semantic Search\n",
    "\n",
    "Semantic search is an advanced information retrieval technique that aims to understand the intent and contextual meaning behind a user's query, rather than simply matching keywords. This approach represents a significant leap forward from traditional keyword-based search methods, enabling more accurate and relevant results.\n",
    "\n",
    "### 1.2 Semantic Search vs. Traditional Keyword-Based Search\n",
    "\n",
    "#### Traditional Keyword-Based Search:\n",
    "- Matches documents containing the exact keywords from the query.\n",
    "- Often requires users to know the right keywords to find relevant information.\n",
    "- Can miss relevant documents that use synonyms or related concepts.\n",
    "- May return irrelevant results if keywords are used in different contexts.\n",
    "\n",
    "#### Semantic Search:\n",
    "- Understands the meaning behind the query and finds documents that are semantically related.\n",
    "- Allows users to use natural language queries.\n",
    "- Can find relevant documents even if they don't contain the exact query terms.\n",
    "- Considers the context of the search and the user's intent.\n",
    "\n",
    "### Example:\n",
    "Consider a user searching for \"apple\". \n",
    "\n",
    "A traditional search might return results about:\n",
    "- The fruit\n",
    "- Apple Inc. (the tech company)\n",
    "- Apple records (the record label)\n",
    "- Recipes containing apples\n",
    "- News articles mentioning \"apple\" in various contexts\n",
    "\n",
    "In contrast, a semantic search would attempt to understand the context:\n",
    "- If the user has recently been browsing tech websites, it might prioritize results about Apple Inc.\n",
    "- If the query is \"how to cook an apple pie\", it would understand that the apple in question is the fruit.\n",
    "- If the query is \"apple stock price\", it would interpret this as relating to the company's financial information.\n",
    "\n",
    "## 2. Key Concepts in Semantic Search\n",
    "\n",
    "### 2.1 Vector Space Model (VSM)\n",
    "\n",
    "The Vector Space Model is a fundamental concept in information retrieval and forms the basis for many semantic search techniques. In VSM, we represent text data (words, documents, queries) as vectors in a high-dimensional space. This mathematical representation allows us to perform various operations and comparisons on text data.\n",
    "\n",
    "Key points about VSM:\n",
    "\n",
    "- Each dimension in the vector space typically corresponds to a term in the vocabulary.\n",
    "- Documents and queries are represented as vectors in this space.\n",
    "- The similarity between documents, or between a query and a document, can be measured by the proximity of their vector representations.\n",
    "\n",
    "Mathematically, we can represent this as follows:\n",
    "\n",
    "Let $D = \\{d_1, d_2, ..., d_n\\}$ be a set of documents and $T = \\{t_1, t_2, ..., t_m\\}$ be the set of terms in the vocabulary.\n",
    "\n",
    "A document $d_i$ is represented as a vector:\n",
    "\n",
    "$$d_i = (w_{i1}, w_{i2}, ..., w_{im})$$\n",
    "\n",
    "where $w_{ij}$ is the weight of term $t_j$ in document $d_i$.\n",
    "\n",
    "The VSM allows us to translate the problem of determining the relevance of a document to a query into a problem of measuring the similarity between vectors. This forms the foundation for many more advanced techniques in semantic search.\n",
    "\n",
    "### 2.2 Embeddings and Representation of Text\n",
    "\n",
    "Embeddings are dense vector representations of words, phrases, or entire documents that capture semantic information. The key idea behind embeddings is that words or documents with similar meanings should have similar vector representations.\n",
    "\n",
    "Characteristics of embeddings:\n",
    "\n",
    "- They are typically low-dimensional (compared to the size of the vocabulary), dense vectors.\n",
    "- Similar meanings result in vectors that are close to each other in the embedding space.\n",
    "- They can capture complex relationships between words, such as analogies.\n",
    "\n",
    "For example, in a well-trained word embedding space:\n",
    "- The vector for \"king\" - \"man\" + \"woman\" should be close to the vector for \"queen\".\n",
    "- The vector for \"Paris\" - \"France\" + \"Italy\" should be close to the vector for \"Rome\".\n",
    "\n",
    "These embeddings form the backbone of many modern semantic search systems, allowing for more nuanced understanding and comparison of text data.\n",
    "\n",
    "## 3. Mathematical Principles of Semantic Search\n",
    "\n",
    "### 3.1 Vector Representations\n",
    "\n",
    "As mentioned earlier, in semantic search, we represent words, phrases, or entire documents as vectors in a high-dimensional space. This is a crucial step that allows us to apply mathematical operations to text data.\n",
    "\n",
    "Formally, each document or query is represented as a vector $\\mathbf{v} \\in \\mathbb{R}^n$, where $n$ is the dimensionality of the embedding space.\n",
    "\n",
    "The choice of how to create these vector representations is a key factor in the performance of a semantic search system. Various methods exist, from simple techniques like bag-of-words to more advanced approaches like word embeddings and contextual embeddings.\n",
    "\n",
    "### 3.2 Similarity Metrics\n",
    "\n",
    "Once we have vector representations, we need ways to measure how similar or different these vectors are. This is crucial for determining which documents are most relevant to a given query. Several similarity metrics are commonly used in semantic search:\n",
    "\n",
    "#### 3.2.1 Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors. It's widely used because it's efficient to calculate and is not affected by the magnitude of the vectors (only their direction matters).\n",
    "\n",
    "$$\\text{cosine\\_similarity}(\\mathbf{v_1}, \\mathbf{v_2}) = \\frac{\\mathbf{v_1} \\cdot \\mathbf{v_2}}{\\|\\mathbf{v_1}\\| \\|\\mathbf{v_2}\\|}$$\n",
    "\n",
    "where $\\cdot$ denotes the dot product and $\\|\\mathbf{v}\\|$ is the Euclidean norm of vector $\\mathbf{v}$.\n",
    "\n",
    "#### 3.2.2 Euclidean Distance\n",
    "\n",
    "Euclidean distance measures the straight-line distance between two points in Euclidean space. In the context of semantic search, a smaller distance indicates greater similarity.\n",
    "\n",
    "$$\\text{euclidean\\_distance}(\\mathbf{v_1}, \\mathbf{v_2}) = \\|\\mathbf{v_1} - \\mathbf{v_2}\\| = \\sqrt{\\sum_{i=1}^n (v_{1i} - v_{2i})^2}$$\n",
    "\n",
    "#### 3.2.3 Dot Product\n",
    "\n",
    "The dot product is another simple and efficient way to measure similarity. For normalized vectors, it's equivalent to cosine similarity.\n",
    "\n",
    "$$\\text{dot\\_product}(\\mathbf{v_1}, \\mathbf{v_2}) = \\mathbf{v_1} \\cdot \\mathbf{v_2} = \\sum_{i=1}^n v_{1i} v_{2i}$$\n",
    "\n",
    "### 3.3 Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. It's based on the idea that:\n",
    "- Words that appear frequently in a document are important to that document (term frequency).\n",
    "- But words that appear frequently across many documents are less discriminative (inverse document frequency).\n",
    "\n",
    "Mathematically, for a term $t$ in a document $d$:\n",
    "\n",
    "$$TF(t,d) = \\frac{\\text{Number of times } t \\text{ appears in } d}{\\text{Total number of terms in } d}$$\n",
    "\n",
    "$$IDF(t) = \\log\\frac{\\text{Total number of documents}}{\\text{Number of documents containing } t}$$\n",
    "\n",
    "$$TF\\text{-}IDF(t,d) = TF(t,d) \\times IDF(t)$$\n",
    "\n",
    "TF-IDF can be used to create document vectors, where each dimension corresponds to a term's TF-IDF score. This method provides a good baseline for many information retrieval tasks and is still relevant in semantic search, often used in combination with more advanced techniques.\n",
    "\n",
    "### 3.4 Word Embeddings\n",
    "\n",
    "Word embeddings are dense vector representations of words that capture semantic meaning.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "\n",
    "Let $V$ be the vocabulary size and $d$ be the embedding dimension. The embedding matrix $E \\in \\mathbb{R}^{V \\times d}$ contains a $d$-dimensional vector for each word in the vocabulary.\n",
    "\n",
    "For a word $w$, its embedding $e_w$ is:\n",
    "\n",
    "$$e_w = E[w] \\in \\mathbb{R}^d$$\n",
    "\n",
    "Word2Vec uses two main architectures:\n",
    "\n",
    "1. Continuous Bag of Words (CBOW): Predicts a target word given its context.\n",
    "2. Skip-gram: Predicts the context given a target word.\n",
    "\n",
    "For the Skip-gram model, the objective is to maximize:\n",
    "\n",
    "$$\\frac{1}{T} \\sum_{t=1}^T \\sum_{-c \\leq j \\leq c, j \\neq 0} \\log p(w_{t+j}|w_t)$$\n",
    "\n",
    "where $c$ is the size of the context window, $T$ is the number of words in the corpus, and $p(w_{t+j}|w_t)$ is modeled using the softmax function:\n",
    "\n",
    "$$p(w_O|w_I) = \\frac{\\exp(e_{w_O}^T e_{w_I})}{\\sum_{w' \\in V} \\exp(e_{w'}^T e_{w_I})}$$\n",
    "\n",
    "Here, $w_I$ is the input word and $w_O$ is the output (context) word.\n",
    "\n",
    "### 3.5 Document Embeddings\n",
    "\n",
    "Document embeddings extend the idea of word embeddings to entire documents. A simple approach is to average the word embeddings of all words in the document:\n",
    "\n",
    "$$d = \\frac{1}{N} \\sum_{i=1}^N e_{w_i}$$\n",
    "\n",
    "where $N$ is the number of words in the document and $e_{w_i}$ is the embedding of the $i$-th word.\n",
    "\n",
    "More sophisticated models like Doc2Vec learn document embeddings directly, similar to Word2Vec but with an additional document vector.\n",
    "\n",
    "### 3.6 Latent Semantic Analysis (LSA)\n",
    "\n",
    "LSA is a technique that uncovers the underlying semantic structure in a document-term matrix using Singular Value Decomposition (SVD). It's based on the idea that words used in similar contexts tend to have similar meanings.\n",
    "\n",
    "Mathematically, LSA applies SVD to the term-document matrix:\n",
    "\n",
    "$$X = U\\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $X$ is the term-document matrix\n",
    "- $U$ and $V$ are orthogonal matrices\n",
    "- $\\Sigma$ is a diagonal matrix of singular values\n",
    "\n",
    "By keeping only the $k$ largest singular values, we obtain a low-rank approximation that captures the most important semantic relationships. This reduces noise and reveals the latent semantic structure of the documents.\n",
    "\n",
    "## 4. Models Used in Semantic Search\n",
    "\n",
    "Semantic search leverages various models to understand and process text data. These models range from traditional statistical approaches to advanced neural network architectures. Understanding these models is crucial for grasping how semantic search systems work and how they've evolved over time.\n",
    "\n",
    "### 4.1 TF-IDF Model\n",
    "\n",
    "While not a deep learning technique, TF-IDF (Term Frequency-Inverse Document Frequency) is still relevant in many semantic search applications and forms a good starting point for understanding more complex models.\n",
    "\n",
    "#### How TF-IDF Works in Semantic Search:\n",
    "\n",
    "1. **Document Representation**: Each document is represented as a vector where each element corresponds to a term in the vocabulary. The value of each element is the TF-IDF score of that term in the document.\n",
    "\n",
    "2. **Query Processing**: The user's query is also converted into a TF-IDF vector.\n",
    "\n",
    "3. **Similarity Calculation**: The similarity between the query vector and document vectors is calculated, often using cosine similarity.\n",
    "\n",
    "4. **Ranking**: Documents are ranked based on their similarity to the query.\n",
    "\n",
    "#### Advantages of TF-IDF in Semantic Search:\n",
    "\n",
    "- Simple and computationally efficient\n",
    "- Provides a good baseline for many information retrieval tasks\n",
    "- Can be combined with more advanced techniques for improved performance\n",
    "\n",
    "#### Limitations:\n",
    "- Doesn't capture word order or context\n",
    "- Cannot understand synonyms or related concepts that don't share exact terms\n",
    "\n",
    "### 4.2 Word2Vec\n",
    "\n",
    "Word2Vec is a groundbreaking model that learns vector representations of words (word embeddings) from large corpora of text.\n",
    "\n",
    "#### How Word2Vec Works in Semantic Search:\n",
    "\n",
    "1. **Training**: Word2Vec is trained on a large corpus of text to learn word embeddings.\n",
    "\n",
    "2. **Document Representation**: Documents are represented by combining the embeddings of their constituent words (e.g., by averaging).\n",
    "\n",
    "3. **Query Processing**: The query is similarly converted into a vector by combining word embeddings.\n",
    "\n",
    "4. **Similarity Calculation**: As with TF-IDF, similarity between query and document vectors is calculated.\n",
    "\n",
    "#### Advantages in Semantic Search:\n",
    "- Captures semantic relationships between words\n",
    "- Can handle synonyms and related concepts\n",
    "- Enables more nuanced similarity calculations\n",
    "\n",
    "#### Limitations:\n",
    "- Static embeddings don't account for context-dependent word meanings\n",
    "- Limited by the quality and diversity of the training corpus\n",
    "\n",
    "### 4.3 BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT, introduced by Google in 2018, represents a significant leap forward in natural language processing and has had a profound impact on semantic search.\n",
    "\n",
    "#### How BERT Works in Semantic Search:\n",
    "\n",
    "1. **Pre-training**: BERT is pre-trained on a large corpus of text using two main tasks:\n",
    "   - Masked Language Modeling (MLM): Predicting masked words in a sentence.\n",
    "   - Next Sentence Prediction (NSP): Predicting if one sentence follows another.\n",
    "\n",
    "2. **Fine-tuning**: The pre-trained BERT model is then fine-tuned on specific tasks, including semantic search.\n",
    "\n",
    "3. **Contextualized Embeddings**: BERT generates contextualized embeddings for words and sentences, meaning the same word can have different representations depending on its context.\n",
    "\n",
    "4. **Document and Query Representation**: Both documents and queries are passed through BERT to generate their respective embeddings.\n",
    "\n",
    "5. **Similarity Calculation**: As with previous models, similarity between query and document embeddings is calculated to rank results.\n",
    "\n",
    "#### Mathematical Formulation:\n",
    "\n",
    "For an input sequence $X = [x_1, ..., x_n]$, BERT produces contextualized representations $H = [h_1, ..., h_n]$ where each $h_i \\in \\mathbb{R}^d$ is a $d$-dimensional vector.\n",
    "\n",
    "The core of BERT is the self-attention mechanism:\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "where $Q$, $K$, and $V$ are query, key, and value matrices derived from the input embeddings.\n",
    "\n",
    "#### Advantages in Semantic Search:\n",
    "- Captures complex, context-dependent meanings of words and phrases\n",
    "- Can handle long-range dependencies in text\n",
    "- Achieves state-of-the-art performance on many NLP tasks, including semantic search\n",
    "\n",
    "#### Limitations:\n",
    "- Computationally intensive, which can be challenging for real-time applications\n",
    "- Requires significant amounts of data and computational resources for training\n",
    "\n",
    "### 4.4 Transformer-based Models in Semantic Search\n",
    "\n",
    "The success of BERT has led to the development of numerous other transformer-based models, each with its own strengths and applications in semantic search.\n",
    "\n",
    "#### 4.4.1 Encoder-Only Models\n",
    "\n",
    "Examples: BERT, RoBERTa, ALBERT\n",
    "\n",
    "These models focus on understanding and representing input text. They are particularly effective for tasks that require deep comprehension of text, such as document classification or relevance ranking in semantic search.\n",
    "\n",
    "#### 4.4.2 Decoder-Only Models\n",
    "\n",
    "Examples: GPT (Generative Pre-trained Transformer), GPT-2, GPT-3\n",
    "\n",
    "While primarily designed for text generation, decoder-only models can be adapted for semantic search tasks. They excel at tasks like query expansion or generating descriptive search snippets.\n",
    "\n",
    "#### 4.4.3 Encoder-Decoder Models\n",
    "\n",
    "Examples: T5 (Text-to-Text Transfer Transformer), BART\n",
    "\n",
    "These models can be used for various subtasks in semantic search, such as query reformulation or document summarization.\n",
    "\n",
    "### 4.5 Why Different Model Architectures Matter in Semantic Search\n",
    "\n",
    "Understanding the differences between encoder-only, decoder-only, and encoder-decoder models is crucial in semantic search because each architecture has its strengths and is suited to different aspects of the search process:\n",
    "\n",
    "1. **Encoder-Only Models (e.g., BERT)**: \n",
    "   - Best for: Understanding the meaning of queries and documents\n",
    "   - Use in semantic search: Generating high-quality embeddings for both queries and documents, which can be directly compared for relevance ranking\n",
    "\n",
    "2. **Decoder-Only Models (e.g., GPT)**:\n",
    "   - Best for: Generating human-like text\n",
    "   - Use in semantic search: Query expansion (generating alternative phrasings for a query), creating detailed search snippets, or even generating natural language explanations of search results\n",
    "\n",
    "3. **Encoder-Decoder Models (e.g., T5)**:\n",
    "   - Best for: Transforming input text into output text\n",
    "   - Use in semantic search: Query reformulation, translating queries between languages in cross-lingual search, or summarizing documents for more informative search results\n",
    "\n",
    "## 5. The Semantic Search Process\n",
    "\n",
    "### 5.1 Indexing\n",
    "\n",
    "Indexing is the process of preparing documents for efficient search and retrieval.\n",
    "\n",
    "#### Steps involved:\n",
    "1. **Text Preprocessing**: Cleaning and normalizing text (e.g., lowercasing, removing punctuation, stemming/lemmatization).\n",
    "2. **Feature Extraction**: Converting text into numerical features (e.g., TF-IDF vectors, word embeddings).\n",
    "3. **Dimensionality Reduction**: Optionally reducing the dimensionality of feature vectors (e.g., using techniques like PCA or t-SNE).\n",
    "4. **Index Structure Creation**: Organizing document vectors in a structure that allows for fast retrieval (e.g., inverted index, vector database).\n",
    "\n",
    "### 5.2 Query Processing\n",
    "\n",
    "When a user submits a query, it undergoes processing to match the format of the indexed documents.\n",
    "\n",
    "#### Steps involved:\n",
    "1. **Query Preprocessing**: Similar to document preprocessing (cleaning, normalization).\n",
    "2. **Query Expansion**: Optionally expanding the query with synonyms or related terms.\n",
    "3. **Query Vectorization**: Converting the query into a vector representation.\n",
    "\n",
    "### 5.3 Retrieval\n",
    "\n",
    "This stage involves finding documents that are potentially relevant to the query.\n",
    "\n",
    "#### Steps involved:\n",
    "1. **Initial Retrieval**: Using efficient algorithms to retrieve a subset of potentially relevant documents.\n",
    "2. **Similarity Calculation**: Computing similarity scores between the query vector and document vectors.\n",
    "\n",
    "### 5.4 Ranking\n",
    "\n",
    "The retrieved documents are ordered based on their relevance to the query.\n",
    "\n",
    "#### Steps involved:\n",
    "1. **Score Calculation**: Computing a relevance score for each retrieved document.\n",
    "2. **Ranking Algorithm**: Ordering documents based on their scores and potentially other factors.\n",
    "\n",
    "### 5.5 Result Presentation\n",
    "\n",
    "The final step involves presenting the ranked results to the user in a meaningful way.\n",
    "\n",
    "#### Steps involved:\n",
    "1. **Snippet Generation**: Creating concise summaries of each result.\n",
    "2. **Result Clustering**: Optionally grouping similar results together.\n",
    "3. **Diversity Promotion**: Ensuring a diverse set of results is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4534b61-8544-453c-a997-81639ba7b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af728a26-643f-4fe3-8bb8-bd34c3d90b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"The apple is a sweet fruit.\",\n",
    "    \"Apple designs and sells consumer electronics.\",\n",
    "     \"Apple's latest software update includes new features.\",\n",
    "    \"She picked an apple from the tree in her backyard.\"\n",
    "]\n",
    "query = \"Apple's stock price increased after the product launch.\"\n",
    "\n",
    "def display_results(model_name, similarity_scores, documents):\n",
    "    print(f\"\\n{model_name} Similarity Scores:\")\n",
    "    for idx, score in enumerate(similarity_scores):\n",
    "        print(f\"Document {idx+1}: {score:.4f} -> {documents[idx]}\")\n",
    "    most_relevant_doc_index = similarity_scores.argmax()\n",
    "    print(f\"\\nMost Relevant Document: {documents[most_relevant_doc_index]}\\n\")\n",
    "\n",
    "def tfidf_model(documents, query):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarity_scores = cosine_similarity(query_vec, tfidf_matrix).flatten()\n",
    "    display_results(\"TF-IDF\", similarity_scores, documents)\n",
    "\n",
    "def word2vec_model(documents, query):\n",
    "    tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "    w2v_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    \n",
    "    def average_vector(text):\n",
    "        vectors = [w2v_model.wv[word] for word in text.lower().split() if word in w2v_model.wv]\n",
    "        return sum(vectors) / len(vectors) if vectors else torch.zeros(100)\n",
    "\n",
    "    doc_embeddings = [average_vector(doc) for doc in documents]\n",
    "    query_embedding = average_vector(query)\n",
    "    similarity_scores = [cosine_similarity([query_embedding], [doc_emb]).flatten()[0] for doc_emb in doc_embeddings]\n",
    "    display_results(\"Word2Vec\", torch.tensor(similarity_scores), documents)\n",
    "\n",
    "def bert_model(documents, query):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def embed_text(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "        \n",
    "    doc_embeddings = [embed_text(doc) for doc in documents]\n",
    "    query_embedding = embed_text(query)\n",
    "    similarity_scores = [cosine_similarity(query_embedding, doc_emb).flatten()[0] for doc_emb in doc_embeddings]\n",
    "    display_results(\"BERT\", torch.tensor(similarity_scores), documents)\n",
    "\n",
    "def transformer_model(documents, query):\n",
    "    transformer_model = pipeline('feature-extraction', model='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "    def embed_text(text):\n",
    "        outputs = transformer_model(text)\n",
    "        return torch.tensor(outputs[0]).mean(dim=0).unsqueeze(0)\n",
    "\n",
    "    doc_embeddings = [embed_text(doc) for doc in documents]\n",
    "    query_embedding = embed_text(query)\n",
    "    similarity_scores = [cosine_similarity(query_embedding, doc_emb).flatten()[0] for doc_emb in doc_embeddings]\n",
    "    display_results(\"Transformer\", torch.tensor(similarity_scores), documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "057d59c2-685b-43aa-b539-2991f02ac275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple's stock price increased after the product launch.\n",
      "\n",
      "TF-IDF Similarity Scores:\n",
      "Document 1: 0.4791 -> The apple is a sweet fruit.\n",
      "Document 2: 0.1254 -> Apple designs and sells consumer electronics.\n",
      "Document 3: 0.1150 -> Apple's latest software update includes new features.\n",
      "Document 4: 0.3170 -> She picked an apple from the tree in her backyard.\n",
      "\n",
      "Most Relevant Document: The apple is a sweet fruit.\n",
      "\n",
      "\n",
      "Word2Vec Similarity Scores:\n",
      "Document 1: 0.2895 -> The apple is a sweet fruit.\n",
      "Document 2: -0.0648 -> Apple designs and sells consumer electronics.\n",
      "Document 3: 0.3926 -> Apple's latest software update includes new features.\n",
      "Document 4: 0.3193 -> She picked an apple from the tree in her backyard.\n",
      "\n",
      "Most Relevant Document: Apple's latest software update includes new features.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mypc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a78bb17d754ec29a16cbfd66e4d3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  45%|####5     | 199M/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Similarity Scores:\n",
      "Document 1: 0.8255 -> The apple is a sweet fruit.\n",
      "Document 2: 0.8286 -> Apple designs and sells consumer electronics.\n",
      "Document 3: 0.8947 -> Apple's latest software update includes new features.\n",
      "Document 4: 0.8363 -> She picked an apple from the tree in her backyard.\n",
      "\n",
      "Most Relevant Document: Apple's latest software update includes new features.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb2ba6f945d4553a05b7759dac9bec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mypc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mypc\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4929a732aba4db6a66326ea7f328a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2a09903a9f43a887482aba157bed9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f071a309f14038a9a34aae1bdc703a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4a8b3bf20841c382c7f3f9f1bc8c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e040cb7bc4431cac4d6017b68cfe12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformer Similarity Scores:\n",
      "Document 1: 0.3928 -> The apple is a sweet fruit.\n",
      "Document 2: 0.4849 -> Apple designs and sells consumer electronics.\n",
      "Document 3: 0.4689 -> Apple's latest software update includes new features.\n",
      "Document 4: 0.2841 -> She picked an apple from the tree in her backyard.\n",
      "\n",
      "Most Relevant Document: Apple designs and sells consumer electronics.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mypc\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(query)\n",
    "\n",
    "tfidf_model(documents, query)\n",
    "word2vec_model(documents, query)\n",
    "bert_model(documents, query)\n",
    "transformer_model(documents, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3f529-73a7-4e75-a98b-e9a6eab3b142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
